# Teamworkâ€¯AI â€“ LLMâ€¯Researchâ€¯AgentÂ (v1)

A productionâ€‘style research assistant that answers any question inâ€¯â‰¤â€¯80â€¯words and **always** cites its sources.  
Runs endâ€‘toâ€‘end **offline** for CI, upgrades to real webâ€‘searchâ€¯+â€¯GPTâ€‘3.5â€‘Turbo when you export the relevant API keys.

> **Pipeline** â€“Â Generateâ€¯â†’â€¯Searchâ€¯â†’â€¯Reflect (â‰¤â€¯2â€¯loops)â€¯â†’â€¯Synthesize  
> **Stack** â€“Â PythonÂ 3.11 Â· Docker Â· LangGraph Â· OpenAI API Â· FastAPI Â· RedisÂ (cache) Â· OpenTelemetry Â· Prometheus Â· Serper/Bing

---

## 1Â â€“Â Architecture at a Glance

```mermaid
flowchart LR
    subgraph User
        Q[CLIÂ /Â HTTPÂ /Â WS<br/>Question]
    end
    subgraph LangGraph
        A[Generate<br/>queries]
        B[WebÂ Search<br/>(BingÂ /Â SerperÂ â†’Â Redis)]
        C[Reflect<br/>(slotÂ filler)]
        D{need_more?}
        E[Synthesize<br/>â‰¤80Â w & cites]
    end
    subgraph Infra
        R[(Redis)]
        OTel[(OTel traces)]
        Prom[(Prom metrics)]
    end

    Q --> A --> B --> C --> D
    D -- yes âŸ³2 --> B
    D -- no  --> E --> Q

    B <--> |1â€¯h cache| R
    A & B & C & E --> OTel & Prom

```

---

## 2Â â€“Â LocalÂ Dev

### 2.1Â Plain Python

# âŠ clone & install
    git clone https://github.com/AbhiRaa/llm-research-agent.git
    cd llmâ€‘researchâ€‘agent && python -m venv .venv && source .venv/bin/activate
    pip install -r requirements.txt

# â‹ stubâ€‘mode (works offline)
    PYTHONPATH=src python -m agent.cli "Who won the 2022 FIFA World Cup?"

### 2.2Â Docker

# Build the image (once)
    docker compose build agent

# Bring up supporting services (redis + otelâ€‘collector)
    docker compose up -d redis otel-collector agent

# Run the agent image interactively
        docker compose run --rm agent python -m agent.cli "Explain vector databases inÂ 3Â sentences"

---

## 3Â â€“Â HTTPÂ Streaming Endâ€‘points

| Type          | URL                          | QuickÂ test                                                                |
| ------------- | ---------------------------- | ------------------------------------------------------------------------- |
| **SSE**       | `GET /api/stream?question=â€¦` | `curl -N "http://localhost:8001/api/stream?question=Who+invented+Docker"` |
| **WebSocket** | `ws://â€¦/api/ws?question=â€¦`   | `npx wscat -c "ws://localhost:8001/api/ws?question=What+is+RAG"`          |

---

## 4Â â€“Â RedisÂ CacheÂ Cheatâ€‘sheet

### 4.1Â Connect to Redis running viaâ€¯dockerâ€‘compose

    # open an interactive redisâ€‘cli in the same network
    docker exec -it agent-redis-1 redis-cli

### 4.2Â List cached queries

    KEYS web_search:*            # show all cached search keys
    GET  web_search:('query',)   # inspect one entry
    TTL  web_search:('query',)   # seconds remaining (â‰ˆ3600)

### 4.3Â Flush everything (âš ï¸Â clears all data)

    FLUSHALL

(You can also run docker compose exec redis redis-cli FLUSHALL  from the shell.)

---

## 5 - Running the chat UI (webâ€‘agent/)

The repository includes a very small Viteâ€¯+â€¯React frontâ€‘end that talks to the agentâ€™s streaming API.
Directory layout (topâ€‘level):

    llm-research-agent/
    â”œâ”€ src/          â† all Python backâ€‘end code
    â”œâ”€ web-agent/    â† the React / Vite frontâ€‘end
    â””â”€ dockerâ€‘compose.yml

### 5.1 - Local dev (hotâ€‘reload)

    # install JS deps
    cd web-agent
    npm install         # or: pnpm install / yarn

    # start the dev server
    npm run dev

Open http://localhost:5173 â€“ youâ€™ll see a singleâ€‘column chat window.
Ask a question; the UI connects to ws://localhost:8001/api/ws and streams tokens as they arrive.

## 6 - Testing Matrix

| What                         | Command                                                                   | Notes                                                             |                                 |
| ---------------------------- | ------------------------------------------------------------------------- | ----------------------------------------------------------------- | ------------------------------- |
| **Unit / integration tests** | `pytest -q`                                                               | 8 deterministic cases (timeout, 429 retry, twoâ€‘round loop, etc.). |                                 |
| **Manual CLI**               | `python -m agent.cli "â€¦" `                                                | Prints pretty JSON.                                               |                                 |
| **HTTPÂ Streaming**           | `curl -N "http://localhost:8001/api/stream?question=Who+invented+Docker"` | Serverâ€‘sent events (`token` & `done`).                            |                                 |
| **WebSocket**                | `npx wscat -c "ws://localhost:8001/api/ws?question=What+is+RAG?"`         | Same payloads over WS.                                            |                                 |
| **Prometheus**               | \`curl [http://localhost:8000/metrics](http://localhost:8000/metrics)     | head\`                                                            | Histogram & counters per phase. |
| **Traces (local)**           | `docker compose up -d otelâ€‘collector jaeger` then hit CLI                 | View trace at `http://localhost:16686` (Jaeger UI).               |                                 |

---

## 7 - DesignÂ Highlights

| Concern                    | Decision                                                                                                               |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| **Deterministic CI**       | Stub LLM & mock search guarantee tests run without internet or keys.                                                   |
| **Retry & latency budget** | 1â€¯s timeout wrapper + exponential backâ€‘off (2â€¯retries) around search; LLM calls inherited from LangChain.              |
| **Caching**                | JSONâ€‘serialisable wrapper stores LangChain `Document`s in Redis (`agent.cache`).                                       |
| **Observability**          | OTel spans for each LangGraph node; Prom countersÂ +Â histograms; both safe in pytest/CI.                                |
| **Extensibility**          | Graph edges & routing are dataâ€‘driven â†’ easy to insert Embedding/RAG or multiple Reflect passes.                       |
| **Failure modes**          | Any exception inside a node â†’ spanÂ `status=ERROR` but pipeline continues with stub/defaults, so the CLI never crashes. |

---

## 8 - ExtensionÂ Ideas (roadâ€‘map)

| Idea                                          | Effort | Notes                                                                                 |                   |
| --------------------------------------------- | ------ | ------------------------------------------------------------------------------------- | ----------------- |
| **StructuredÂ function calling in Synthesize** | â—â—â—‹    | Enforce JSON schema & real URLs (partly prototyped).                                  |                   |
| **Vectorâ€‘store RAG**                          | â—â—â—‹    | Index cached search docs; Reflect can query embeddings instead of extra Google calls. |                   |
| **Tool selection (HNSW, wiki, arXiv)**        | â—â—â—    | Add more search â€œtoolsâ€ & use an LLMâ€‘router to pick.                                  |                   |
| **Frontend UI**                               | â—â—‹â—‹    | Minimal React / HTMX page consuming SSE for realâ€‘time tokens.                         |                   |
| **Auth & billing**                            | â—â—â—‹    | Rateâ€‘limit per APIâ€‘key, record token usage, integrate Stripe.                         |                   |
| **Fineâ€‘grained metrics**                      | â—â—‹â—‹    | Attach `model.name`, `cache.hit`, \`provider="bing                                    | serper"\` labels. |

---

## 9 - TroubleshootingÂ / FAQ

| Symptom                                                  | Fix                                                                                                            |
| -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **â€œHTTPÂ 429 from Bingâ€**                                 | The retry logic should backâ€‘off automatically; if it persists, unset `BING_API_KEY` so Serper/mock takes over. |
| **Spans but no Jaeger UI traces**                        | Ensure `OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318` and that the collector container is reachable. |
| **Prometheus port already in use when runningâ€¯`pytest`** | Tests spawn multiple agents; `observability.py` silently skips starting a duplicate server (safe to ignore).   |
| **CLI hangs for >1â€¯s in *search***                       | Likely internet outage; timeout triggers mock fallback after 1â€¯s.                                              |

---

## 10 - Directoryâ€¯Layout

    src/agent/
        â”œâ”€ cache.py           # Redis serialization wrapper
        â”œâ”€ cli.py             # CLI entryâ€‘point
        â”œâ”€ graph.py           # LangGraph builder
        â”œâ”€ nodes.py           # Generate / Search / Reflect / Synthesize
        â”œâ”€ tools.py           # Bing / Serper + mock
        â”œâ”€ observability.py   # OTel + Prom bootstrap
        â””â”€ server.py          # FastAPI SSE / WebSocket
    tests/                   # 8 verified scenarios
    docker-compose.yml       # agent + redis + otelâ€‘collector
    README.md                # you are here

---

## 11 - Bonusâ€‘item implementationâ€¯status

| Bonus feature                                            | Implemented?              | Notes                                                                                                                                                                       |
| -------------------------------------------------------- | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ“¡ **SSEâ€¯/â€¯WebSocket streaming**                         | âœ…Â Done                    | `src/agent/server.py` exposes `/api/stream` (SSE) and `/api/ws` (WS) that stream incremental *token* and *done* events.                                                     |
| â™»ï¸ **Redis LRU cache for query results**                 | âœ…Â Done                    | `@cached` decorator in `src/agent/cache.py` stores JSONâ€‘serialisable results with a TTL (defaultâ€¯1â€¯h). Becomes a noâ€‘op when `REDIS_URL` is unset or Redis is unreachable.   |
| ğŸ“ˆ **OpenTelemetry traces + Prometheus metrics**         | âœ…Â Done                    | `src/agent/observability.py` initialises tracing (console + optional OTLP exporter) and two Prometheus instruments (`agent_requests_total`, `agent_phase_latency_seconds`). |
| ğŸ’¬ **Minimal React/Vite frontâ€‘end**                      | âœ…Â Done                    | `web/` (orâ€¯`ui/`) folder serves a Viteâ€‘built chat UI that connects via the WS endpoint; start with `npm run dev`.                                                           |
| ğŸ“‘ **OpenAI functionâ€‘calling to constrain *Synthesize*** | âŒÂ **Not implemented yet** | Current synthesize node uses plain chat completion; adding a strict functionâ€‘call wrapper is still on the backlog.                                                          |
| ğŸ”— **Slotâ€‘Aware Reflect loop**                           | âœ…Â Done                    | `reflect_node` emits `need_more`Â +Â `new_queries`; router loops back to *Search* until all required slots are filled or `MAX_ITER` reached.                                  |


::contentReference[oaicite:0]{index=0}





